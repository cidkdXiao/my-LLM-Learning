# LLM Learning Notes
## Architecture of LLM
### Transformer
- Introduction to Transformer: https://jalammar.github.io/illustrated-transformer/
- AnnotatedTransformer.ipynb: Contains whole pipeline of transformer, including model architecture, model training and additional components like BPE. (Copy from https://colab.research.google.com/drive/1diArrXHFLPswqdbbkolnV9xgWCeVVUZO#scrollTo=9a429510)
- linear_attention_transformer.py (Whole project can be seen at: https://github.com/lucidrains/linear-attention-transformer/tree/master)
- rope.py (From: https://nn.labml.ai/transformers/rope/index.html)
## Pretraning
### Crawler
- craw_web.py (来自小红书博主 R.Z 的学习笔记，https://www.xiaohongshu.com/user/profile/62a1c8ae000000001b029899?xsec_token=ABTTzF2veUI5ptXF2n-Mii5341oxLP1__8ag4Bi0yVdEc%3D&xsec_source=pc_search)
- MediaCrawler (https://github.com/NanmiCoder/MediaCrawler#)
### Data Clean